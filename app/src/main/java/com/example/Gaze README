This is Ese Omene. The following is a description of how I created the series of programs in the robot commonly known
as Gaze Aversion. To start I would need to explain what Gaze Aversion is and how it works. If you could honestly care
less (which... I can't lie, I wouldn't blame you for) Then you can skip to the 7th bullet point where I summarize
all of this.


1. Description of Gaze Aversion
    To figure out what Gaze Aversion is, I was told by Mr.Michaelis to study the research article entitled
    'Conversational Gaze Aversion for HumanLike Robots' by Sean Andrist, Xiang Zhi Tan, Michael Gleicher, and Bilge Mutlu.
    From this paper, I gathered that Gaze Aversion is 'the intentional redirection away from the face of aninterlocutor'
    which is important for three basic things: 'signaling cognitive effort, regulating a conversationâ€™s
    intimacy level, and managing the conversational floor'. Because of the naturalness and human-esque qualities this
    feature can provide, Mr.Michaelis thought it would be imparitive to utilize this behavior and incorporate it into our
    own temi-robot.

2. Description of what the Robot should do with Gaze Aversion
    So the robot should be able to perform 3 specific types of gazes:
    - Cognitive Gaze : the ability to talk while giving the impression you are thinking or being cognitive of your response
    - Intimacy Regulated Gaze : the ability to talk while recognizing level of interaction you should have with someone else
    - Floor Management Gaze : the ability to talk while being aware of the surroundings around you
    The article explains that Cognitive Gazes usually happen right before and at the start of a person's dialog, Intimacy Regulated
    Gaze happens during the middle of a conversation, and Floor Management Gaze can happen during the tail end of the conversation.
    Along with this, Mr. Michaelis also advices me to create methods that will allow us to perform each type of Gaze individually.
    With all this in mind, it was time to get started.

3. Creating the Gaze Feature: Making the Images
    So in order to make the robot Gaze in the first place, I decided to implement a different animation method than for making the facial expression.

    For animating facial expressions, I would usually make a FacialExpressionGraph with all the connected nodes (sprites/drawables) and so
    when looking for an animation, it would sort through the graph from the point where it was at, and use DFS to pick up the drawables until
    it finds the right sprite. Then it will create a motion picture using the sprites it found in the path.

    This was going to be the strategy that I wanted, but after trying to make the sprites for the pupils to move, it ended up becoming a pain, so
    I just decided to create xml files for the eyes without the pupils to replace the eyes created before, and then I created another xml file
    for the pupil in order to move it around. Then I added imageViews for the pupils to be layered on top of the left and right eyes in the temi_face
    layout.

    Now we have the new drawables for the eyes and the layout to display them.

4. Creating the Gaze Feature: Animating the Gaze: EyeGaze
    I needed a class that would help in moving the pupils UP, DOWN, LEFT, and RIGHT. So that's pretty much what this class does.

    I created a series of final float values that act as the boundaries for the pupils to move in. They are too lengthy to put in here,
    so if you want to see them, go to the EyeGaze class.

    The methods in the class can be divided into two types
    - VISUAL:
        - revealPupils(): reveal the pupils in the temi_face layout
        - hidePupils(): hide the pupils in the temi_face layout
    - DIRECTIONAL:
        - sendPupilsUp(): move pupils upwards
        - sendPupilsDown(): move pupils downwards
        - sendPupilsLeft(): move pupils left
        - sendPupilsRight(): move pupils right

    The way the pupil animation works is by using the .animate() method for the IMAGEVIEWS housing the pupils. I ended up with much
    more smoother transitions than when I was trying to draw and animate the sprites for it using the FacialExpressionGraph.

    CAUTION: There is a catch to doing it this way (Yeah yeah sue me or whatever):
        - Two Animation Methods:
          Because there are two animation methods, I would need to switch the imageViews from the eyes used in gazing to the eyes used in creating
          facial expressions. This means if you wanted to make the face change it's expression in the middle of gazing, it would end up looking like
          the face had 2 pupils (mainly because the sprite has a built in pupil and the pupil imageView will be layered on top of that). It honestly
          looks ugly and so as a precaution, I made it so that you are unable to change the FACIAL EXPRESSION until the Face is no longer
          in GazeMode.
        - There is only pupils and eyes made for the Happy/Neutral expression:
          if you wanted to gaze using other expressions, you would need to make those yourselves. I started working on some of the other eyes
          but I haven't gotten around to finishing, let alone implementing those into the animations. If this needs to be done, it would have to
          be something you implement for yourself.

5. Creating the Gaze Feature: GazeType Enum
    In determining how to make the right computations based on the gaze type, I decided to make an Enum called GazeType that will store
    the appropriate gaze variables. These variables include:
    - the likelihood the robot will face up
    - the likelihood the robot will face left
    - the likelihood the robot will face down
    - the likelihood the robot will face right
    - the mean duration of specific gaze
    - the standard deviation of duration
    This way, when we make the functions that require a specific gaze type and it's qualities, we can use these as our definitions.

6. Creating the Gaze Feature: Gaze Class
    Gazing is such a complicated feature to implement, which means it couldn't be in a subclass of Face without making the code look
    over crowded and bogged up. So I will be making the Gaze class, and either extend the Gaze class to the Face class, or add an instance
    of the Gaze class to the Face class. The Gaze class will ultimately need to have several capabilities in order for it to work:
    - Data Members: These are constant static values that will help with identifying Gaze values:
        CANCEL: -1
        STRAIGHT: 0
        COGNITIVE: 1
        INTIMACY_REGULATION: 2
        FLOOR_MANAGEMENT: 3
        PERFORM_ALL: 4
    - Private Methods: These are going to be the helper methods that the Gaze Class will use to perform some of it's functions
        getDirection(GazeType): get directions of where the robot should face
        getDuration(GazeType): calls the durationOfGaze() method based off of the gaze type
        durationOfGaze(int mean, int sd): calculates the durations based off the mean and standard deviation
    - Public Methods: These are going to be the methods that the user can actually call
        performGaze(): performs the gaze based off of the gaze type.
        ** I don't use this method. To see how I perform gaze functions skip to the Nth bullet point. **

7. Creating the Gaze Feature: Threads: GazeThread
    Threading/Multithreading can seem daunting at first (If you never really did it) but once you learn the ins and outs you'd be able to
    figure it out.

    For the Gaze Aversion to work, not only do we need the helper functions to determine direction of gaze and duration of gaze, but we also need
    the gazing to happen in the background while the user is able to interact with the robot. When you put too many many computations in the main
    Thread (AKA the UI Thread) then the app can run slow and eventually crash. That's why it's important to use worker Threads to perform the
    necessary computations while the UI Thread is doing it's own thing.

    To achieve this, I created a private class in the Gaze.java file called GazeThread that extends Thread. This thread will also need a
    handler to handle the messages that tell the Thread what to do in the background.
    GazeThread will use the following things to work:
        - Handler
        - Looper

    Handler is something that a Thread will use when running to handle 'messages' in the MessageQueue. I created a handler called
    'handler' (yes, I am VERY original) and this handler handles the static int messages listed in the Gaze class
    ('COGNITIVE', 'INTIMACY_REGULATED', etc). This is defined in the handleMessage() method of the handler.

    Looper is just what is used inside the Thread's run() method. The Looper is used to loop through the MessageQueue and checks if there are
    any messages that need to be handled by the handler.

    Anyway, once a specific gaze is called, it will create a AsyncTask called GazeTask that will perform the gaze based on the message sent.
    But I'm getting ahead of myself. I need to explain what the AsyncTask is first.

8. Creating the Gaze Feature: AsyncTasks: GazeTask
    Even though the worker thread was able to help in performing computations in the background thread, we also needed to effect the UI Thread
    in order to show that the robot was gazing (for example, if the robot is performing the cognitive gaze, we would need to show that the robot is
    thinking visually as well). However, the worker thread is unable to control imageViews and other things that have been made in the UI Thread.
    This means that you would have to perform the imageView changes AFTER the thread was finished handling it's messages, which could cause
    delays in the interaction, which is not what we want.

    This is where AsyncTask come in.

    AsyncTasks are simply Tasks that give you the flexibility to run something on a background thread while updating the UI thread. To do this,
    AsyncTasks will need to know 3 things
        - Params: What type of values are you putting into the execute() method
        - Progress: What type of values should be published to the UI Thread
        - Result: What type of value should returned as the result of the background computation.

    The AsyncTask that I created uses GazeType, String and Void as its values
        - GazeType will hold the necessary information needed for the duration, and finding the direction
        - String will be needed to determine which direction to look at. ("LEFT", "RIGHT", "UP", "DOWN")
        - Void just because we don't need to do anything else with the computation.

    For AsyncTask, there are a lot of Override functions that you can use to help you, but for the purpose of the Gaze feature, I really only
    needed 3:
        - onPreExecute(): This will let the Face know that it is in GazeMode and so nothing else should effect the face's emotions but the gazing
        - doInBackground(String... gazeTypes): This will calculate the direction and duration of gaze based on gazeType, and after publishing results
                                               it will sleep for the duration.
        - onProgressUpdate(String... values) : This will get the published results and perform the actual gaze based on the results.
        - onCancelled(): This will stop the gaze feature whenever the gaze feature was cancelled.
        - onCancelled(Void aVoid): This will also call the handleCancelled() method

    Then I made a function that handled the onCancelled methods called handleCancelled() (I swear these names are just too good) and this
    function basically stops the gaze feature whenever the gaze feature was cancelled, and lets the Face know that it's no longer in GazeMode


9. Implementing to the Face Class
    I created an instance of the GazeThread in the Face class called gazeThread (The creative names just keep on coming).
    Then I create a public method called "changeGazeTo(int gazeType)" that instantiate the GazeThread if it
    haven't already, then it sends a gazeType message to the now running thread to perform the gaze. This is only one way to use the Gaze Feature
    you can also create an instance of the Gaze Class and call "performGaze(int gazeType)" (See 6th bullet point)
    Method 1: Face -> changeGazeTo() -> GazeThread.run()
    Method 2: Gaze -> performGaze() -> GazeThread.run()
    As long as the face in TransitionDemo is initialized, both methods are 100% ok to use. I personally like to use the face just because we want these
    features to be incorporated into the Face class anyway. This was just a remnant of code that used to test the feature. It can be deleted if you
    also find it worthless :(

10. Conclusion
    So to summarize:
        I created sprites to move the pupils around and added it to the worker thread
        I created a class that animates the pupils
        I created Enums for all the specific gazeTypes
        I created data members needed for handling messages in a worker thread
        I created the private functions to get the durations and direction of gazes based on gaze type
        I created the GazeThread that checks for the gaze message and performs the AsyncTasks based on gaze type
        I created the GazeTask that performs the direction and duration along with facial changes base on the gaze type
        I instantiated the GazeThread inside of the Face class in order to call upon the gaze feature from the Face class.

    So this is pretty much the main points I have for the Gaze Aversion feature, however, there are still many things that need to be implemented if not
    here, than in the Face class.

    For example:
        Sprites still need to be made for the thoughtful gazes
        Functions to implement these type of facial expressions still needs to be created (in the Face class)

    With that in mind, I might be making more updates to the Gaze and Face classes in the future before my internship is over so that I can fix these
    issues. Those edits will be documented below the Conclusion of each README file.

                                                                                                               -Ese Omene