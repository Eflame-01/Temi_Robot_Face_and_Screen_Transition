This is Ese Omene. The following is a description of how I created the face portion of this screen transition
app and created some of its functionality. To start I need to explain what the face is. If you could honestly
care less (which... I can't lie, I wouldn't blame you for) you can skip to the 5th bullet point where I summarize
all of this.


1. Description and History of Face
    The face is recreation of the F6 robot in a Kalegina paper "Characterizing the Design Space of Rendered Robot Faces".
    In this paper, there is a particular robot that Mr. Michaelis found to be best suited for our temi-robot: test subject F6.
    In case you are not reading this from the src folder and cannot access the f6_robot_face.png file as a result,
    the face is shown in the paper. In that paper, the robot face was shown to exhibit the following qualities:
        - Maturity
        - Friendliness
        - Trustworthiness
        - Human-like
        - Masculine
    In terms of like to dislike ratio, this robot face was received more positively than negatively, so the robot face became the
    prime candidate for our temi robot.

    In terms of description, the face is simply two huge eyes with thin eyebrows slightly over them, no nose, and a mouth slightly
    thicker than the eyebrows towards the bottom. The face was black, and the pupils (or space between the white thick circles for his eyes)
    were also black. It is a simple but nice robot face, which I believed help people to project nice qualities onto it.

2. Description of What the Face Should Do
    The face needed to be more than static: it also needed to be capable of expressing emotion. After much contemplation,
    Mr. Michaelis decided the best way for the emotions to be displayed should be based on the Bazo paper
    "Design and Testing of a Hybrid Expressive Face for a Humanoid Robot". In this paper, they have about 7 emotions:
        - Happy
        - Surprised
        - Angry
        - Tired
        - Sad
        - Afraid
        - Neutral
    However, in order to give each expression a new look, what they did was change the positioning, angle, and shape of the facial features
    (I know that sounds obvious but hear me out). Instead of having to make each emotional FACE animation UNIQUE, you can instead create
    TRANSITIONS to certain FACIAL FEATURES, creating more realistic facial expression transitions, with considerably fewer lines of code and sprites.

    To elaborate: Lets say you wanted to create animations for one expression to another in a brute force kind of way.
        If you wanted to go from one expression to another expression, you would need to create 6 different transitions from one face to the next.
        Then, in order to go from one of those faces to another face, you would need an additional 5 unique transitions. This would be extremely costly,
        (7! or 5040 animation transitions to be exact) not only in terms of drawing, coding and time, but for my sanity. (And I kinda need my sanity...)

        Instead, the Bazo paper shows us that each emotion has particular facial characteristics we can look out for, and manipulate those instead. Now,
        if we wanted to go from one expression to another, we would need to check, for example, if the eyebrows should be raised, lowered, frown, or rise,
        and animate the face accordingly. Not only will this give us the same realism that we would get from animating each expression transition,
        but we also have more flexibility to add more expressions and change how we want those expressions to look later on.

3. Creating the Face: Layouts and Sprites
    Creating the face to display was easy in concept but still tedious to do.
    - I took the image of the F6 robot and blew it up into the Android Studio layout files to take the measurements of the
      eyebrows, eyes, and mouth in relation to the screen/layout. (see temi_face_new_measurements.xml if you need to see how I took measurements)
    - After getting the sizes, I used the graphics design application Krita to create the following:
        - base eye, eyebrow, and mouth
        - eyes with smaller pupil, eyes with slightly lowered top eyelid, eyes with even more lowered eyelid, and eyes responsible for blinking
        - eyebrows that would elevate upwards, eyebrows that would be lowered downwards, eyebrows that rise upwards, and eyebrows that set downwards.
        - mouth that opened, mouth that closed, mouth that smiled, mouth that frowned.
    - After creating the images I put them inside the res->drawable folder in the android project. (Yes most of those pictures are the transitions for
      the facial features, imagine how hard it would've been if it was 7! facial transitions I needed to make).
    - After putting the images in the src folder, I created my own layout and used the ratio I got from the temi_face_new_measurements for my own
      face (this is the temi_face.xml layout. The reason why the layout looks blank is because it doesn't start off with any background image).

4. Creating the Face: Code
    The code for the actual face is actually pretty cool (it was my first time trying something like this). The code for creating the face are the following
        - Face.java : responsible for utilizing all the other .java files and their functions
          - Mood.java : responsible for housing the values for a particular emotion
          - FacialExpressionGraph.java : responsible for holding graphs of sprites needed to animate one facial feature to another.
            - Path.java : responsible for holding a specific path in the Graph necessary for creating the animation.
            - Node.java : holds one sprite and edges to other sprites/Nodes in the Graph
            - Edge.java : holds two sprites/Nodes that are closely related to each other in the Graph
          - BlinkTask.java : responsible for making the eyes blink.

    In order to give you the most comprehensive but not complicated overview of how I implemented this, I'm gonna explain to you the code in the following
    order:
     - Mood
     - Node
     - Edge
     - Path
     - FacialExpressionGraph
     - Face

    Mood:
        Mood.java is a ENUM that contains the static final values of all the emotion's facial features. Each Mood has different values
        overall, but some of the values for certain facial features may be the same amongst certain moods (the eyes for happy are the same as the eyes
        for neutral).
        The Mood enum has 5 data members/parameters: left_eyebrow, right_eyebrow, left_eye, right_eye, and mouth. These are int values that contain the
        ids of the drawable files in the res->drawable folder that I placed the sprite images in.

    Node:
        Node.java is a CLASS that contains the drawableID and list of other Nodes that it is connected to using the Edge class.
        Node has 2 parameters: drawableID and adjacencyList. The drawableID is the id for the drawable image sprite in res->drawable.
        adjacency list is an ArrayList of all the Edges the Node has. Node.java only has the function addEdge(), which adds/replaces an Edge
        in the adjacencyList.

    Edge:
        Edge.java is a CLASS that shows the destination of the edge in question.
        Edge has 2 data parameters: weight, and destinationID. weight is the amount (in int) the line/edge has to the Node, and the destinationID is the value
        of the destination Node's drawableID. Edge.java only has the function sameDestination(), which helps the Node class figure out if their are
        multiple edges in their list that point to the same destinationID. The function checks if the values of he destinationID are the same.

    Path:
        Path.java is a CLASS that holds the list of sprites/frames needed to create the animation for a facial feature.
        Path has 2 parameters: weightOfPath, and drawableFrames. the weightOfPaths is the value (in int) for the total weight of the path. The drawableFrames
        is a list of all the drawable frames needed for the animation in order from the starting frame to the ending frame. Path only has the function
        addToPath(), which just adds the drawableID to the drawableFrames

    FacialExpressionGraph:
        FacialExpressionGraph.java is a CLASS that uses the three classes: Node, Edge, and Path to create a graph for a particular facial feature.
        FacialExpressionGraph.java has 2 data members: nodeLookup and pathLookup. nodeLookup is used to find a particular node in a HashMap. PathLookup
        is used to find a particular Path in a HashSet.

        Overview of FacialExpressionGraph.java:
            The Graph will first need to be created after you make an instance of it. it is created using the create[INSERT FACIAL FEATURE] method.
                This method calls createEdgeInGraph method to create edges between one drawableID and another. It calls this method until
                all of the facial features are added and connected to at least one other node, creating a complete Graph of that facial feature
                (each create[INSERT FACIAL FEATURE] is already hardcoded)
            After the Graph is created, to get an animation, you would need to call the animateFacialFeature method.
                This method checks to see what facial feature you want to animate, checks which animation you need by getting the
                current mood of your face, and the new mood you want to transition to.
                After it does that, it calls findPathsInGraphs method to get the paths that lead to that transition.
                After it finds the paths, it will call the findBestPathInGraph method to get the best suited path.
                After this, it will create the animation using the frames it collects from the best path.

        FacialExpressionGraph.java has the following methods:
            createEdgeInGraph() - Creates Edges between source node and destination node.
            findPathsInGraph() - Creates Paths to one facial feature image to another.
            hasPathsDFS() - Helper function to findPathsInGraph() using Depth First Search.
            findBestPathInGraph() - Gets the path out of the list that has the least amount of weight.
            animateFacialFeature() - Animates a particular facial feature using the other methods above.
            createLeftEyebrowExpressionGraph() - Creates the graph for the left eyebrow.
            createRightEyebrowExpressionGraph() - Creates the graph for the right eyebrow.
            createEyeExpressionGraph() - Creates the graph for the left and right eye.
            createMouthExpressionGraph() - Creates the graph for the mouth.

    Face:
        Face.java incorporates all of these classes to show the expressions of one emotion to another emotion,
        and blinking animations.
        Face has the following data members:
            LEFT_EYEBROW - 0
            RIGHT_EYEBROW - 1
            LEFT_EYE - 2
            RIGHT_EYE - 3
            MOUTH - 4
            activity - the Activity that the face is inside
            mood - current mood of the face
            facialExpressionGraph[] - holds all graphs for each facial expression
            resource[] - holds all the original facial features
            imageView[] - holds the imageViews for each facial feature
            AnimationDrawable[] - holds the Animation for each facial feature
            baseColor - int value of color of robot's face
            porterDuffColorFilter - colorMode for customizing robot
            mainHandler - helps performs functions on another Runnable

        The Face takes in the Activity that the Face is in, and the Handler that the Face is in. Once it does that, it will set up all of the
        data members listed above needed to use the Face class. (These are the setter/getter functions, as well as the setAll functions. You
        do not need to modify them unless you change the data members)

        The Face has other private methods that help with maintaining the Face's functionality:
            resetFace() - sets the facial features to whatever is in their resource[], usually their current mood.
            (in case they were set to an animationDrawable)

            resetColor() - used to maintain the color of the images when they change by coloring in the imageView itself.

            rateOfBlink() - used to calculate when the robot should blink using a CDF. (Research into this was based on a study
            Mr.Michaelis gave to me (I do not know the name of the paper nor do I have a copy)

            startWaitingToBlink() - used to set a Timer that goes off based on the rateOfBlink(). When the timer is up, the robot face will blink.

            blink() - used to make the robot blink autonomously.

        The Face has one more public method that the user can use to manipulate the emotions of the robot. This method is called changeFaceTo()
        changeFaceTo is a method that takes in the Mood enum value, the value in which you want your robot to change it's face into. The function works
        in this way:
           After checking if the mood is different from the current mood the robot is already in, it will then reset the face into it's imageView
           in case the imageViews were changed to an AnimationDrawable

           Then it uses the FacialExpressionGraph for each facial feature to create a custom animation for each animation and put it in
           AnimationDrawable[].

           After it gets the animations needed to change the emotion, it uses a for loop to change the imageView's backgrounds to the AnimationDrawable

           After it sets the imageView background to show the animations, it will then use another for loop to play each animation, changing the expression
           from one expression to another expression.

           Then once the expression is changed, the mood will be set to the new mood, and the color will be reset so that it doesn't change during the
           expression transition.

        The Face also has a private class inside of it called BlinkTask. BlinkTask is a subclass of TimerTask and implements the Runnable interface.
        BlinkTask is used to scheduled Timed Tasked on another "Thread", in this case, it is used so that the robot can blink periodically without
        freezing the app. Doing it this way was the first solution that I tried that actually worked, so I didn't check any other options.

        Anyway, since BlinkTask implements Runnable, it has one function: run(). For our run method, we use the mainHandler (You thought I forgot about
        mainHandler) to post or call the functions blink(), and startWaitingToBlink(). The blink function will cause the robot to blink, and the
        startWaitingToBlink() will create a new instance of the BlinkTask, along with a new blinkRate given by the rateOfBlink() method.
        (The method looks like this:
            Timer t = new Timer();
            t.schedule(TimerTask yourTimerTask, long millisBeforeTaskBegins);
        )
        This will create a new BlinkTask that will call the blink() and startWaitingToBlink() methods again, essentially starting the cycle over again.

5. Conclusion
    So to summarize:
        I created the sprites for the appropriate emotions (R.drawable)
        I measured and recreated the face for our temi-robot (R.layout.temi_face, R.layout.temi_face_new_measurements)
        I used those sprites to make values for the Mood enum (Mood.java)
        I created graphs of sprites for each facial feature that maps to other facial expressions (Node.java, Edge.java, Path.java, FacialExpressionGraph.java)
        I created a class that uses the Mood values and FacialExpressionGraphs to manipulate the ImageViews in the layout to create realistic emotions. (Face.java)
        I further implemented a TimerTask that would be used to have the robot blink regularly (Face.java)

    So this is everything that I worked on so far in terms of the actual Face. I will (should) have another README about how I used this app and incorporated
    it into Michael's Screen Transition App (which you may or may not be reading this from).


6. EDITS to Face Class
    I changed some things to the Face class that will be used to help facilitate other features besides changing the color and facial expressions.
    - I simplified the Face class' data member facialExpressionGraph[] (an array of FacialExpressionsGraph) to already be defined with their
      respective graphs for each facial feature.
    - I extended the Gaze class to perform gaze functions through the gaze class ( I may later just create the instance of the Gaze class inside the
      Face class and use the functions by calling the instantiated class.)
    - I have also done some simplification and edits that allow only three methods to be used by the user besides the getters an setter:
        - changeFaceTo()
        - changeColorTo()
        - changeGazeTo()
        - changeToBlink()
    - I added a boolean data member called gazeMode that is true whenever the Face is gazing and it false when it is done gazing. This is so that
        you are unable to change the face while the robot is gazing.
    - I added two more protected methods called setEyesToPupils() and setEyesToNoPupils() that will be called before and after the Face is in GazeMode
        respectively

                                                                                                                            -Ese Omene
